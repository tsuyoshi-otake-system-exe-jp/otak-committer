# Code Conventions

## 1. Project Structure and Dependencies

### 1.1 Basic Structure

```
src/
├── commands/       # Command implementations
│   ├── generateCommit.ts  # Generate commit messages
│   ├── generateIssue.ts   # Issue generation
│   └── generatePR.ts      # PR generation
├── services/       # Service layer
│   ├── base.ts           # Base service and factory implementations
│   ├── branch.ts         # Branch management interface and utilities
│   ├── git.ts           # Git operations (diff retrieval, state management)
│   ├── github.ts        # GitHub API (PR/Issue creation)
│   ├── issueGenerator.ts # Issue generation logic
│   ├── openai.ts        # OpenAI API (text generation)
│   └── prompt.ts        # Prompt generation and management
├── languages/      # Multilingual support
│   ├── asian.ts           # Asian languages
│   ├── european.ts        # Western languages
│   ├── middleEastern.ts   # Middle Eastern languages
│   ├── english.ts         # English prompts
│   ├── japanese.ts        # Japanese prompts
│   └── index.ts           # Aggregation of language settings
├── types/         # Type definitions
│   ├── git.ts     # Git-related types
│   ├── github.ts  # GitHub-related types
│   ├── language.ts # Language-related types
│   └── messageStyle.ts # Message style definitions
└── utils/         # Utility functions
    ├── fileSelector.ts # File selection UI
    └── preview.ts     # Preview display
```

### 1.2 Dependencies of Main Components

#### Entry Point
- `extension.ts`
  - Initializes the extension and loads configurations
  - Registers commands (generateCommit, generateIssue, generatePR)
  - Manages language settings (`LANGUAGE_CONFIGS`) and message styles

#### Commands Dependencies
- `generateCommit.ts`
  - → `git.ts`: Retrieve diffs
  - → `openai.ts`: Generate messages
  - → `messageStyle.ts`: Style definitions
  - → `commitGuide.ts`: Prefix information

- `generateIssue.ts`
  - → `issueGenerator.ts`: Issue generation logic
  - → `git.ts`: Repository analysis
  - → `github.ts`: Issue creation
  - → `fileSelector.ts`: File selection UI
  - → `preview.ts`: Preview display

- `generatePR.ts`
  - → `github.ts`: Branch information and PR creation
  - → `git.ts`: Local operations
  - → `openai.ts`: Title and body generation
  - → `preview.ts`: Preview display

### 1.3 Service Layer Architecture

#### Base Classes and Interfaces
- `BaseService`
  - 基本的なサービス機能の提供
  - コンフィグ管理とエラーハンドリング
  - 状態検証とリソース解放
  ```typescript
  class BaseService {
    protected config: ServiceConfig;
    async ensureConfig(key: keyof ServiceConfig, promptMessage: string): Promise<boolean>;
    protected handleError(error: any): never;
    protected showError(message: string, error?: any): void;
    protected validateState(condition: boolean, message: string): void;
    async dispose(): Promise<void>;
  }
  ```

- `BaseServiceFactory`
  - サービスインスタンスの生成と依存関係の管理
  - 初期化エラーのハンドリング
  ```typescript
  class BaseServiceFactory<T> {
    async create(config?: Partial<ServiceConfig>): Promise<T>;
    protected validateDependencies(...services: (BaseService|undefined)[]): Promise<boolean>;
    protected handleInitError(error: any, message: string): undefined;
  }
  ```

#### Service Implementations
- `GitService`
  - Gitリポジトリ操作の提供
  - ファイル差分と状態の取得
  - テンプレート検出機能
  
- `GitHubService`
  - GitHub API操作の実装
  - リポジトリ情報の検出と管理
  - Issue/PR作成機能の提供
  
- `OpenAIService`
  - OpenAI APIとの連携
  - 各種生成処理の実装
  - APIキー検証機能

- `IssueGeneratorService`
  - Issue生成ロジックの統合
  - ファイル分析と内容生成
  - 複数サービスの連携管理

#### Branch Management
- `BranchManager` (Interface)
  ```typescript
  interface BranchManager {
    getCurrentBranch(): Promise<string | undefined>;
    getBranches(): Promise<string[]>;
  }
  ```

- `BranchSelector` (Utility)
  - ブランチ選択UIの提供
  - 現在のブランチを考慮したソート機能

#### Prompt Management
- `PromptService`
  - 各種プロンプトの生成と管理
  - テンプレートの適用
  - 多言語対応の統合

## 2. Coding Guidelines

### 2.1 TypeScript Settings
- `strict`: true
- `noImplicitAny`: true
- `esModuleInterop`: true
- `skipLibCheck`: true

### 2.2 Service Implementation Pattern

```typescript
export class ServiceImplementation extends BaseService {
    constructor(config?: Partial<ServiceConfig>) {
        super(config);
    }
    
    async initialize(): Promise<void> {
        await this.ensureConfig('requiredKey', 'Configuration message');
        this.validateState(condition, 'Validation message');
    }
    
    async operation(): Promise<Result> {
        try {
            // Implementation
        } catch (error: any) {
            this.handleError(error);
        }
    }
}
```

### 2.3 Factory Implementation Pattern

```typescript
export class ServiceFactory extends BaseServiceFactory<Service> {
    async create(config?: Partial<ServiceConfig>): Promise<Service> {
        try {
            const service = new Service(config);
            await service.initialize();
            return service;
        } catch (error) {
            return this.handleInitError(error, 'Service initialization failed');
        }
    }
}
```

### 2.4 Error Handling

```typescript
try {
    await operation();
} catch (error: any) {
    this.handleError(error);
}
```

## 3. Feature Implementation

### 3.1 Commit Message Generation
1. Retrieve Git diff (via `git.ts`)
2. Generate text using the OpenAI API (`openai.ts`)
3. Apply the message style
4. Reflect the changes in the SCM input field

### 3.2 Issue Generation
1. Select the type of Issue
2. Choose the files to analyze (via `fileSelector.ts`)
3. Generate a preview (via `preview.ts`)
4. Create the GitHub Issue (via `github.ts`)

### 3.3 PR Generation
1. Retrieve branch information and diff
2. Generate the title and body using OpenAI
3. Display a preview for confirmation
4. Execute the PR creation

### 3.4 Configuration Management

```typescript
const config = vscode.workspace.getConfiguration('otakCommitter');
const language = config.get<string>('language') || 'japanese';
const messageStyle = config.get<MessageStyle>('messageStyle') || 'normal';
```

## 4. Testing Guidelines

### 4.1 Service Tests

```typescript
describe('ServiceImplementation', () => {
    let service: ServiceImplementation;
    
    beforeEach(() => {
        service = new ServiceImplementation();
    });
    
    it('should initialize correctly', async () => {
        await expect(service.initialize()).resolves.not.toThrow();
    });
    
    it('should handle errors appropriately', async () => {
        await expect(service.operation()).rejects.toThrow('Expected error');
    });
});
```

### 4.2 Configuration Tests

```typescript
test('Configuration loads', () => {
    const config = vscode.workspace.getConfiguration('otakCommitter');
    assert.ok(config.get('setting'));
});
```

## 5. Build and Packaging

```bash
npm run vscode:prepublish && npx @vscode/vsce package
```

## 6. Environment Variables

- `OPENAI_API_KEY`: OpenAI API key
- `GITHUB_TOKEN`: GitHub personal access token

## 7. Dependency Management

- `simple-git`: Git client
- `openai`: OpenAI API SDK
- `@types/*`: Type definition packages
- `@octokit/rest`: GitHub API client

## 8. Code Review Criteria

1. サービス層の適切な分離と責任の明確化
2. ファクトリーパターンの正しい実装
3. エラーハンドリングの一貫性
4. 設定値の適切な利用
5. パフォーマンスへの配慮

## 9. Performance Considerations

1. サービスの遅延初期化
2. リソースの適切な解放
3. API呼び出しの最適化
4. キャッシュの活用

## 10. Security

1. API キーの安全な管理
2. トークンの適切な取り扱い
3. 機密情報の保護
4. 入力値の検証

## Knowledge Base

### GPT-4.1
- **Context Window**: Supports up to 1 million tokens (approximately 3 million words)
- **Reasoning Capabilities**: Significantly improved understanding, association, and reasoning with complex information
- **Processing Speed**: Approximately twice as fast as GPT-4
- **Pricing Structure**: Input tokens at $10/million tokens, output tokens at $30/million tokens
- **Performance Improvements**:
  - Enhanced coding capabilities for complex programming challenges, debugging, and code optimization
  - Improved comprehension and summarization of lengthy texts like books and academic papers
  - More accurate interpretation and execution of complex, multi-step instructions
  - Approximately 40% reduction in factual errors, particularly in scientific and technical domains
- **Use Cases**: Analysis and improvement of large codebases, integration and analysis of information from multiple lengthy documents, enhancement of complex question-answering systems

### o3 Model
- **Multimodal Processing**: Integrated processing of text, images, and audio
- **Enhanced Reasoning**: Specialized in complex mathematical problems and logical reasoning
- **Context Understanding**: Ability to understand contexts of up to 1.5 million tokens
- **Web Browsing**: Capability to search and analyze web content in real-time
- **Unique Features**:
  - Seamless integration with external tools and APIs
  - Maintenance and utilization of long-term conversation history
  - Self-learning and improvement based on feedback
  - Pricing: Input at $15/million tokens, output at $50/million tokens
- **Applications**: Scientific research assistance, advanced content creation, corporate decision support systems

### o4-mini
- **Efficiency**: Retains approximately 70% of o3's capabilities while being about 1.5 times faster
- **Resource Optimization**: Operable with fewer computing resources
- **Size Optimization**: Model size reduced to about 40% of o3
- **Context Window**: 500,000 tokens
- **Cost Efficiency**:
  - Pricing: Input at $5/million tokens, output at $15/million tokens
  - Lower server requirements, making it accessible to small and medium-sized enterprises
- **Application Scenarios**: Integration into mobile applications, AI implementation in medium-sized businesses, utilization in educational institutions and NPOs

### Responses API
- **Technical Specifications**:
  - Unified interface for all new models (GPT-4.1, o3, o4-mini)
  - Structured requests in JSON format
  - Customizable output formats (JSON, Markdown, plain text)
  - Batch processing capability for simultaneous queries
- **Developer Features**:
  - Real-time response generation through streaming
  - Detailed error messages and recovery options
  - Flexible API call limit settings
  - Explicit API version specification and backward compatibility
- **Authentication and Security**:
  - Support for API keys, OAuth 2.0, and JWT tokens
  - 256-bit SSL/TLS encryption for all communications
  - Detailed permission settings and IP restrictions
  - Comprehensive API access logs and usage analytics
- **Integration Example**:
  ```python
  import openai
  
  # API authentication setup
  openai.api_key = "your-api-key"
  
  # Model selection and request configuration
  response = openai.Responses.create(
      model="o3",  # Specify model ("gpt-4.1", "o3", "o4-mini")
      prompt="Please explain the principles and applications of quantum computing in detail",
      max_tokens=2000,
      temperature=0.7,
      format="markdown",  # Specify output format
      tools=["web_search"],  # Specify tools (o3 only)
      streaming=True  # Enable streaming
  )
  
  # Response processing
  for chunk in response:
      print(chunk.content, end="")
  ```

## Response Guidelines

1. **Accuracy**: Ensure all technical information provided is accurate and up-to-date as of April 2025.
2. **Completeness**: Cover all relevant aspects of the requested information, including specifications, capabilities, use cases, and limitations.
3. **Clarity**: Structure your responses with clear headings, bullet points, and code examples where appropriate.
4. **Adaptability**: Adjust the technical depth of your explanations based on the user's apparent level of expertise.
5. **Objectivity**: Present factual information without bias, acknowledging both strengths and limitations of the technologies.

Your primary function is to serve as a knowledgeable resource on OpenAI's latest models and APIs, helping users understand their features, differences, and potential applications in various contexts.